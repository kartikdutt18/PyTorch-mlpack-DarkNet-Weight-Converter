{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda7943c5342761411196fb69cfb335ae6a",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layers(in_channels, out_channels):\n",
    "    layer = nn.Sequential(OrderedDict([('conv',nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)),\n",
    "            ('Bn', nn.BatchNorm2d(out_channels)), ('leaky_relu', nn.LeakyReLU(0.01))]))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_layers():\n",
    "    layer = nn.Sequential(OrderedDict([('max_pool', nn.AvgPool2d(kernel_size=2, stride=2))]))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyYOLO(nn.Module):\n",
    "    def __init__(self, num_bboxes=2, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.features = self.make_features()\n",
    "        self.classifier = self.make_classifier(num_bboxes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 7 * 7 * 256)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def make_features(self):\n",
    "        layers = []\n",
    "        layers.append(conv_layers(3, 16))\n",
    "        layers.append(pooling_layers())\n",
    "        out_channels = 16\n",
    "        \n",
    "        for i in range(0, 5):\n",
    "            layers.append(conv_layers(out_channels, out_channels * 2))\n",
    "            layers.append(pooling_layers())\n",
    "            out_channels = out_channels * 2\n",
    "        layers.append(conv_layers(out_channels, out_channels * 2))\n",
    "        out_channels = out_channels * 2\n",
    "        layers.append(conv_layers(out_channels, 256))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def make_classifier(self, num_bboxes, num_classes):\n",
    "        return nn.Sequential(nn.Sequential(nn.Linear(in_features = 256 * 7 * 7, out_features = 1470),\n",
    "                nn.Sigmoid()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyYOLO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 1470])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "model(torch.rand((1, 3, 224 * 2, 224 * 2))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "def make_directory(base_path : str) -> int :\n",
    "    \"\"\"\n",
    "        Checks if a directory exists and if doesn't creates the directory.\n",
    "\n",
    "        Args:\n",
    "        base_path : Directory path which will be created if it doesn't exist.\n",
    "\n",
    "        Returns 0 if directory exists else 1\n",
    "    \"\"\"\n",
    "    if os.path.exists(base_path) :\n",
    "        return 0\n",
    "\n",
    "    # Create the directory since the path doesn't exist.\n",
    "    os.mkdir(base_path)\n",
    "    if os.path.exists(base_path) :\n",
    "        return 0\n",
    "\n",
    "    # Path doesn't exist as well as directory couldn't be created.\n",
    "    print(\"Error : Cannot create desired path : \", base_path)\n",
    "    return 1\n",
    "\n",
    "def generate_csv(csv_name : str, weight_matrix : torch.tensor, base_path : str, transpose = False) -> str :\n",
    "    \"\"\"\n",
    "        Generates csv for weights or bias matrix.\n",
    "\n",
    "        Args:\n",
    "        csv_name : A string name for csv file which will store the weights.\n",
    "        weight_matrix : A torch tensor holding weights that will be stored in the matrix.\n",
    "        base_path : Base path where csv will be stored.\n",
    "    \"\"\"\n",
    "    # Check if base path exists else create directory.\n",
    "    make_directory(base_path)\n",
    "    file_path = os.path.join(base_path, csv_name)\n",
    "    matrix = weight_matrix.numpy().ravel()\n",
    "    np.savetxt(file_path, matrix, fmt='%1.128f')\n",
    "    if transpose:\n",
    "        matrix = weight_matrix.numpy().transpose().ravel()\n",
    "        np.savetxt(file_path, matrix, fmt='%1.128f')\n",
    "        print(\"Transposed\")\n",
    "    return file_path\n",
    "\n",
    "def extract_weights(layer, layer_index, base_path) -> {} :\n",
    "    \"\"\"\n",
    "        Extracts weights, biases and other parameters required to reproduce\n",
    "        the same output.\n",
    "\n",
    "        Args:\n",
    "        layer : An torch.nn object (layer).\n",
    "        layer_index : A string determining name of csv file that will be appended to\n",
    "                      name of layer.\n",
    "                      Eg. if layer = nn.Conv2d and layer_index = 0\n",
    "                          csv_filename = Conv_layer_index.csv\n",
    "        base_path : A string depicting base path for storing weight / bias csv.\n",
    "\n",
    "        Returns dictionary of parameter description and parameters.\n",
    "\n",
    "        Exceptions:\n",
    "        Currently this has only been tested for convolutional and batch-norm layer.\n",
    "    \"\"\"\n",
    "    parameter_dictionary = {}\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # The layer corresponds to Convolutional layer.\n",
    "        # For convolution layer we require weights and biases to reproduce the\n",
    "        # same result.\n",
    "        parameter_dictionary[\"name\"] = \"Convolution2D\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.in_channels\n",
    "        parameter_dictionary[\"output-channels\"] = layer.out_channels\n",
    "        # Assume weight matrix is never empty for nn.Conv2d()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"conv_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"conv_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_channels\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    elif isinstance(layer, nn.BatchNorm2d) :\n",
    "        # The layer corresponds to Batch Normalization layer.\n",
    "        # For batchnorm layer we require weights, biases and running mean and running variance\n",
    "        # to reproduce the same result.\n",
    "        parameter_dictionary[\"name\"] = \"BatchNorm2D\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.num_features\n",
    "        parameter_dictionary[\"output-channels\"] = layer.num_features\n",
    "        # Assume weight matrix is never empty for nn.BatchNorm2d()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"batchnorm_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"batchnorm_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_channels\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        # Assume BatchNorm layer always running variance and running mean.\n",
    "        running_mean_csv = \"batchnorm_running_mean_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 1\n",
    "        parameter_dictionary[\"running_mean_csv\"] = generate_csv(running_mean_csv, \\\n",
    "            layer.running_mean.detach(), base_path)\n",
    "        parameter_dictionary[\"has_running_var\"] = 1\n",
    "        running_var_csv = \"batchnorm_running_var_\" + layer_index + \".csv\" \n",
    "        parameter_dictionary[\"running_var_csv\"] = generate_csv(running_var_csv, \\\n",
    "            layer.running_var.detach(), base_path)\n",
    "    elif (isinstance(layer, nn.Linear)) :\n",
    "        # The layer corresponds to Convolutional layer.\n",
    "        # For convolution layer we require weights and biases to reproduce the\n",
    "        # same result.\n",
    "        parameter_dictionary[\"name\"] = \"Linear\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.in_features\n",
    "        parameter_dictionary[\"output-channels\"] = layer.out_features\n",
    "        # Assume weight matrix is never empty for nn.Linear()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"linear_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path, True)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"linear_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_features\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    else :\n",
    "        # The layer corresponds to un-supported layer or layer doesn't have trainable\n",
    "        # parameter. Example of such layers are nn.MaxPooling2d() and nn.SoftMax.\n",
    "        parameter_dictionary[\"name\"] = \"unknown_layer\"\n",
    "        parameter_dictionary[\"input-channels\"] = 0\n",
    "        parameter_dictionary[\"output-channels\"] = 0\n",
    "        parameter_dictionary[\"has_weights\"] = 0\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        parameter_dictionary[\"weight_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_bias\"] = 0\n",
    "        parameter_dictionary[\"bias_offset\"] = 0\n",
    "        parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    return parameter_dictionary\n",
    "\n",
    "def create_xml_tree(parameter_dictionary : dict, root_tag = \"layer\") -> ElementTree.ElementTree() :\n",
    "    \"\"\"\n",
    "        Creates an XML tree from a dictionary wrapped around root tag.\n",
    "\n",
    "        Args:\n",
    "        parameter_dictionary : Dictionary which will be converted to xml tree.\n",
    "        root_tag : Tag around which elements of dictionary will be wrapped.\n",
    "                    Defaults to \"layer\".\n",
    "    \n",
    "        Returns : ElementTree.ElementTree() object.\n",
    "    \"\"\"\n",
    "    layer = ElementTree.Element(root_tag)\n",
    "    for parameter_desc in parameter_dictionary :\n",
    "        parameter_description = ElementTree.Element(parameter_desc)\n",
    "        parameter_description.text = str(parameter_dictionary[parameter_desc])\n",
    "        layer.append(parameter_description)\n",
    "    return layer\n",
    "\n",
    "def create_xml_file(parameter_dictionary : dict,\n",
    "                    xml_path : str,\n",
    "                    root_tag : str,\n",
    "                    element_tag : str) -> int :\n",
    "    \"\"\"\n",
    "        Appends layer description to xml file and if xml doesn't exist or is empty, \n",
    "        creates an xml file with required headers.\n",
    "\n",
    "        Args:\n",
    "        parameter_dictionary : Dictionary containing layer description.\n",
    "        xml_path : Path where xml file will be stored / created.\n",
    "        root_tag : Tag around which xml file will be wrapped.\n",
    "        element_tag : Tag around which each element in dictionary will be wrapped.\n",
    "    \"\"\"\n",
    "   \n",
    "    if not os.path.exists(xml_path) :\n",
    "        # Create base xml file.\n",
    "        f = open(xml_path, \"w\")\n",
    "        data = \"<\" + root_tag + \">\" + \"</\" + root_tag + \">\"\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "    layer_description = create_xml_tree(parameter_dictionary, element_tag)\n",
    "    xml_file = ElementTree.parse(xml_path)\n",
    "    root = xml_file.getroot()\n",
    "    layer = root.makeelement(element_tag, parameter_dictionary)\n",
    "    root.append(layer_description)\n",
    "    xml_file.write(xml_path, encoding = \"unicode\")\n",
    "    return 0\n",
    "\n",
    "def iterate_over_layers(modules, xml_path, base_path, layer_index, debug : bool) -> int :\n",
    "    \"\"\"\n",
    "        Parses model and generates csv and xml file which will be iterated by C++ translator.\n",
    "    \n",
    "        Args:\n",
    "        modules : PyTorch model for which parameter csv and xml will be created.\n",
    "        xml_path : Directory where xml with model config will be saved.\n",
    "        base_path : Directory where csv will be stored.\n",
    "\n",
    "        Returns 0 if weights are created else return 1.\n",
    "    \"\"\"\n",
    "    for block in modules :\n",
    "        for layer in block :\n",
    "            layer_index += 1\n",
    "            parameter_dict = extract_weights(layer, str(layer_index), base_path)\n",
    "            create_xml_file(parameter_dict, xml_path, \"model\", \"layer\")\n",
    "            if not os.path.exists(parameter_dict[\"weight_csv\"]) and parameter_dict[\"has_weights\"] == 1:\n",
    "                print(\"Creating weights failed!\")\n",
    "                return 1, layer_index\n",
    "            if debug :\n",
    "                print(\"Weights created succesfully for \", parameter_dict[\"name\"], \" layer index :\", layer_index)\n",
    "    return 0, layer_index\n",
    "\n",
    "def parse_model(model, xml_path, base_path, debug : bool) -> int :\n",
    "    \"\"\"\n",
    "        Parses model and generates csv and xml file which will be iterated by C++ translator.\n",
    "    \n",
    "        Args:\n",
    "        model : PyTorch model for which parameter csv and xml will be created.\n",
    "        xml_path : Directory where xml with model config will be saved.\n",
    "        base_path : Directory where csv will be stored.\n",
    "\n",
    "        Returns 0 if weights are created else return 1.\n",
    "    \"\"\"\n",
    "    layer_index = 0\n",
    "    error, layer_index = iterate_over_layers(model.features, xml_path, base_path, layer_index, debug)\n",
    "    if error :\n",
    "        print(\"An error occured!\")\n",
    "        return 1\n",
    "    print(layer_index)\n",
    "    error, layer_index = iterate_over_layers(model.classifier, xml_path, base_path, layer_index, debug)\n",
    "    if error :\n",
    "        print(\"An error occured!\")\n",
    "        return 1\n",
    "    print(layer_index)\n",
    "    if debug :\n",
    "        print(\"Model weights saved! Happy mlpack-translation.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Weights created succesfully for  Convolution2D  layer index : 1\nWeights created succesfully for  BatchNorm2D  layer index : 2\nWeights created succesfully for  unknown_layer  layer index : 3\nWeights created succesfully for  unknown_layer  layer index : 4\nWeights created succesfully for  Convolution2D  layer index : 5\nWeights created succesfully for  BatchNorm2D  layer index : 6\nWeights created succesfully for  unknown_layer  layer index : 7\nWeights created succesfully for  unknown_layer  layer index : 8\nWeights created succesfully for  Convolution2D  layer index : 9\nWeights created succesfully for  BatchNorm2D  layer index : 10\nWeights created succesfully for  unknown_layer  layer index : 11\nWeights created succesfully for  unknown_layer  layer index : 12\nWeights created succesfully for  Convolution2D  layer index : 13\nWeights created succesfully for  BatchNorm2D  layer index : 14\nWeights created succesfully for  unknown_layer  layer index : 15\nWeights created succesfully for  unknown_layer  layer index : 16\nWeights created succesfully for  Convolution2D  layer index : 17\nWeights created succesfully for  BatchNorm2D  layer index : 18\nWeights created succesfully for  unknown_layer  layer index : 19\nWeights created succesfully for  unknown_layer  layer index : 20\nWeights created succesfully for  Convolution2D  layer index : 21\nWeights created succesfully for  BatchNorm2D  layer index : 22\nWeights created succesfully for  unknown_layer  layer index : 23\nWeights created succesfully for  unknown_layer  layer index : 24\nWeights created succesfully for  Convolution2D  layer index : 25\nWeights created succesfully for  BatchNorm2D  layer index : 26\nWeights created succesfully for  unknown_layer  layer index : 27\nWeights created succesfully for  Convolution2D  layer index : 28\nWeights created succesfully for  BatchNorm2D  layer index : 29\nWeights created succesfully for  unknown_layer  layer index : 30\n30\nTransposed\nWeights created succesfully for  Linear  layer index : 31\nWeights created succesfully for  unknown_layer  layer index : 32\n32\nModel weights saved! Happy mlpack-translation.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "parse_model(model, \"./cfg/\" + \"yolov1_tiny\" + \".xml\", \"./models/\" + \"yolov1\" + \"/mlpack-weights/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.rand((1, 3, 224 * 2, 224 * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./../input_tensor.csv'"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "generate_csv(\"../input_tensor.csv\", input_tensor, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./../output_tensor.csv'"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "generate_csv(\"../output_tensor.csv\", output_tensor.detach(), \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "27097662"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "27097662\n",
    "17313024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 16, 448, 448]             448\n       BatchNorm2d-2         [-1, 16, 448, 448]              32\n         LeakyReLU-3         [-1, 16, 448, 448]               0\n         MaxPool2d-4         [-1, 16, 224, 224]               0\n            Conv2d-5         [-1, 32, 224, 224]           4,640\n       BatchNorm2d-6         [-1, 32, 224, 224]              64\n         LeakyReLU-7         [-1, 32, 224, 224]               0\n         MaxPool2d-8         [-1, 32, 112, 112]               0\n            Conv2d-9         [-1, 64, 112, 112]          18,496\n      BatchNorm2d-10         [-1, 64, 112, 112]             128\n        LeakyReLU-11         [-1, 64, 112, 112]               0\n        MaxPool2d-12           [-1, 64, 56, 56]               0\n           Conv2d-13          [-1, 128, 56, 56]          73,856\n      BatchNorm2d-14          [-1, 128, 56, 56]             256\n        LeakyReLU-15          [-1, 128, 56, 56]               0\n        MaxPool2d-16          [-1, 128, 28, 28]               0\n           Conv2d-17          [-1, 256, 28, 28]         295,168\n      BatchNorm2d-18          [-1, 256, 28, 28]             512\n        LeakyReLU-19          [-1, 256, 28, 28]               0\n        MaxPool2d-20          [-1, 256, 14, 14]               0\n           Conv2d-21          [-1, 512, 14, 14]       1,180,160\n      BatchNorm2d-22          [-1, 512, 14, 14]           1,024\n        LeakyReLU-23          [-1, 512, 14, 14]               0\n        MaxPool2d-24            [-1, 512, 7, 7]               0\n           Conv2d-25           [-1, 1024, 7, 7]       4,719,616\n      BatchNorm2d-26           [-1, 1024, 7, 7]           2,048\n        LeakyReLU-27           [-1, 1024, 7, 7]               0\n           Conv2d-28            [-1, 256, 7, 7]       2,359,552\n      BatchNorm2d-29            [-1, 256, 7, 7]             512\n        LeakyReLU-30            [-1, 256, 7, 7]               0\n           Linear-31                 [-1, 1470]      18,441,150\n================================================================\nTotal params: 27,097,662\nTrainable params: 27,097,662\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 2.30\nForward/backward pass size (MB): 158.21\nParams size (MB): 103.37\nEstimated Total Size (MB): 263.87\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(model, (3, 448, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18439680\n",
    "18441150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "18441150"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "18439680 + 1470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20172452"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "1731302 + 18439680 + 1470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.5037, 0.4955, 0.5030,  ..., 0.4940, 0.5045, 0.5004],\n       grad_fn=<SelectBackward>)"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "output_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}