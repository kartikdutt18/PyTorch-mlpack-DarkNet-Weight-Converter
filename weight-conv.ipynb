{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from base import BaseModel\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.data.size(0)\n",
    "        C = x.data.size(1)\n",
    "        H = x.data.size(2)\n",
    "        W = x.data.size(3)\n",
    "        x = F.avg_pool2d(x, (H, W))\n",
    "        x = x.view(N, C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet19(BaseModel):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(Darknet19, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "        ('layer1', nn.Sequential(OrderedDict([\n",
    "            ('conv1_1', nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn1_1', nn.BatchNorm2d(32)),\n",
    "            ('leaky1_1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('maxpool1', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        ]))),\n",
    "        ('layer2', nn.Sequential(OrderedDict([\n",
    "            ('conv2_1', nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn2_1', nn.BatchNorm2d(64)),\n",
    "            ('leaky2_1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('maxpool2', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        ]))),\n",
    "        ('layer3', nn.Sequential(OrderedDict([\n",
    "            ('conv3_1', nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn3_1', nn.BatchNorm2d(128)),\n",
    "            ('leaky3_1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv3_2', nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0, bias=False)),\n",
    "            ('bn3_2', nn.BatchNorm2d(64)),\n",
    "            ('leaky3_2', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv3_3', nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn3_3', nn.BatchNorm2d(128)),\n",
    "            ('leaky3_3', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('maxpool3', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        ]))),\n",
    "        ('layer4', nn.Sequential(OrderedDict([\n",
    "            ('conv4_1', nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn4_1', nn.BatchNorm2d(256)),\n",
    "            ('leaky4_1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv4_2', nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0, bias=False)),\n",
    "            ('bn4_2', nn.BatchNorm2d(128)),\n",
    "            ('leaky4_2', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv4_3', nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn4_3', nn.BatchNorm2d(256)),\n",
    "            ('leaky4_3', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('maxpool4', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        ]))),\n",
    "        ('layer5', nn.Sequential(OrderedDict([\n",
    "            ('conv5_1', nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn5_1', nn.BatchNorm2d(512)),\n",
    "            ('leaky5_1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv5_2', nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0, bias=False)),\n",
    "            ('bn5_2', nn.BatchNorm2d(256)),\n",
    "            ('leaky5_2', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv5_3', nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn5_3', nn.BatchNorm2d(512)),\n",
    "            ('leaky5_3', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv5_4', nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=1, bias=False)),\n",
    "            ('bn5_4', nn.BatchNorm2d(256)),\n",
    "            ('leaky5_4', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv5_5', nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn5_5', nn.BatchNorm2d(512)),\n",
    "            ('leaky5_5', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('maxpool5', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        ]))),\n",
    "        ('layer6', nn.Sequential(OrderedDict([\n",
    "            ('conv6_1', nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn6_1', nn.BatchNorm2d(1024)),\n",
    "            ('leaky6_1', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv6_2', nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0, bias=False)),\n",
    "            ('bn6_2', nn.BatchNorm2d(512)),\n",
    "            ('leaky6_2', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv6_3', nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn6_3', nn.BatchNorm2d(1024)),\n",
    "            ('leaky6_3', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv6_4', nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1, bias=False)),\n",
    "            ('bn6_4', nn.BatchNorm2d(512)),\n",
    "            ('leaky6_4', nn.LeakyReLU(0.1, inplace=True)),\n",
    "            ('conv6_5', nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn6_5', nn.BatchNorm2d(1024)),\n",
    "            ('leaky6_5', nn.LeakyReLU(0.1, inplace=True))\n",
    "        ])))\n",
    "        ]))\n",
    "\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "        ('classifier', nn.Sequential(OrderedDict([\n",
    "        ('conv7_1', nn.Conv2d(1024, 1000, kernel_size=(1, 1), stride=(1, 1))),\n",
    "        ('globalavgpool', GlobalAvgPool2d()),\n",
    "        ('softmax', nn.Softmax(dim=1))\n",
    "        ])))]))\n",
    "\n",
    "        if pretrained:\n",
    "            # self.load_state_dict(model_zoo.load_url(model_paths['darknet19'],  progress=True))\n",
    "            self.load_weight()\n",
    "            print('Model is loaded')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    def load_weight(self):\n",
    "        weight_file = 'weights/darknet19-deepBakSu-e1b3ec1e.pth'\n",
    "        assert len(torch.load(weight_file).keys()) == len(self.state_dict().keys())\n",
    "        dic = {}\n",
    "        for now_keys, values in zip(self.state_dict().keys(), torch.load(weight_file).values()):\n",
    "            dic[now_keys]=values\n",
    "        self.load_state_dict(dic)\n",
    "        print('Weights are loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Weights are loaded!\nModel is loaded\n"
    }
   ],
   "source": [
    "model = Darknet19(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(base_path : str) -> int :\n",
    "    \"\"\"\n",
    "        Checks if a directory exists and if doesn't creates the directory.\n",
    "\n",
    "        Args:\n",
    "        base_path : Directory path which will be created if it doesn't exist.\n",
    "\n",
    "        Returns 0 if directory exists else 1\n",
    "    \"\"\"\n",
    "    if os.path.exists(base_path) :\n",
    "        return 0\n",
    "\n",
    "    # Create the directory since the path doesn't exist.\n",
    "    os.mkdir(base_path)\n",
    "    if os.path.exists(base_path) :\n",
    "        return 0\n",
    "\n",
    "    # Path doesn't exist as well as directory couldn't be created.\n",
    "    print(\"Error : Cannot create desired path : \", base_path)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(csv_name : str, weight_matrix : torch.tensor, base_path : str) -> str :\n",
    "    \"\"\"\n",
    "        Generates csv for weights or bias matrix.\n",
    "\n",
    "        Args:\n",
    "        csv_name : A string name for csv file which will store the weights.\n",
    "        weight_matrix : A torch tensor holding weights that will be stored in the matrix.\n",
    "        base_path : Base path where csv will be stored.\n",
    "    \"\"\"\n",
    "    # Check if base path exists else create directory.\n",
    "    make_directory(base_path)\n",
    "    file_path = os.path.join(base_path, csv_name)\n",
    "    np.savetxt(file_path, weight_matrix.numpy().ravel())\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(layer, layer_index, base_path) -> dict :\n",
    "    \"\"\"\n",
    "        Extracts weights, biases and other parameters required to reproduce\n",
    "        the same output.\n",
    "\n",
    "        Args:\n",
    "        layer : An torch.nn object (layer).\n",
    "        layer_index : A string determining name of csv file that will be appended to\n",
    "                      name of layer.\n",
    "                      Eg. if layer = nn.Conv2d and layer_index = 0\n",
    "                          csv_filename = Conv_layer_index.csv\n",
    "        base_path : A string depicting base path for storing weight / bias csv.\n",
    "\n",
    "        Returns dictionary of parameter description and parameters.\n",
    "\n",
    "        Exceptions:\n",
    "        Currently this has only been tested for convolutional and batch-norm layer.\n",
    "    \"\"\"\n",
    "    parameter_dictionary = {}\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # The layer corresponds to Convolutional layer.\n",
    "        # For convolution layer we require weights and biases to reproduce the\n",
    "        # same result.\n",
    "        parameter_dictionary[\"name\"] = \"Convolution2D\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.in_channels\n",
    "        parameter_dictionary[\"output-channels\"] = layer.out_channels\n",
    "        # Assume weight matrix is never empty for nn.Conv2d()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"conv_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"conv_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_channels\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    elif isinstance(layer, nn.BatchNorm2d) :\n",
    "        # The layer corresponds to Batch Normalization layer.\n",
    "        # For batchnorm layer we require weights, biases and running mean and running variance\n",
    "        # to reproduce the same result.\n",
    "        parameter_dictionary[\"name\"] = \"BatchNorm2D\"\n",
    "        parameter_dictionary[\"input-channels\"] = layer.num_features\n",
    "        parameter_dictionary[\"output-channels\"] = layer.num_features\n",
    "        # Assume weight matrix is never empty for nn.BatchNorm2d()\n",
    "        parameter_dictionary[\"has_weights\"] = 1\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        csv_name = \"batchnorm_weight_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"weight_csv\"] = generate_csv(csv_name, \\\n",
    "            layer.weight.detach(), base_path)\n",
    "        if layer.bias != None:\n",
    "            parameter_dictionary[\"has_bias\"] = 1\n",
    "            parameter_dictionary[\"bias_offset\"] = 0\n",
    "            bias_csv_name = \"batchnorm_bias_\" + layer_index + \".csv\"\n",
    "            parameter_dictionary[\"bias_csv\"] = generate_csv(bias_csv_name, \\\n",
    "                layer.bias.detach(), base_path)\n",
    "        else:\n",
    "            parameter_dictionary[\"has_bias\"] = 0\n",
    "            parameter_dictionary[\"bias_offset\"] = layer.out_channels\n",
    "            parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        # Assume BatchNorm layer always running variance and running mean.\n",
    "        running_mean_csv = \"batchnorm_running_mean_\" + layer_index + \".csv\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 1\n",
    "        parameter_dictionary[\"running_mean_csv\"] = generate_csv(running_mean_csv, \\\n",
    "            layer.running_mean.detach(), base_path)\n",
    "        parameter_dictionary[\"has_running_var\"] = 1\n",
    "        running_var_csv = \"batchnorm_running_var_\" + layer_index + \".csv\" \n",
    "        parameter_dictionary[\"running_var_csv\"] = generate_csv(running_mean_csv, \\\n",
    "            layer.running_var.detach(), base_path)\n",
    "    else :\n",
    "        # The layer corresponds to un-supported layer or layer doesn't have trainable\n",
    "        # parameter. Example of such layers are nn.MaxPooling2d() and nn.SoftMax.\n",
    "        parameter_dictionary[\"name\"] = \"unknown_layer\"\n",
    "        parameter_dictionary[\"input-channels\"] = 0\n",
    "        parameter_dictionary[\"output-channels\"] = 0\n",
    "        parameter_dictionary[\"has_weights\"] = 0\n",
    "        parameter_dictionary[\"weight_offset\"] = 0\n",
    "        parameter_dictionary[\"weight_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_bias\"] = 0\n",
    "        parameter_dictionary[\"bias_offset\"] = 0\n",
    "        parameter_dictionary[\"bias_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_mean\"] = 0\n",
    "        parameter_dictionary[\"running_mean_csv\"] = \"None\"\n",
    "        parameter_dictionary[\"has_running_var\"] = 0\n",
    "        parameter_dictionary[\"running_var_csv\"] = \"None\"\n",
    "    return parameter_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xml_tree(parameter_dictionary : dict, root_tag = \"layer\") -> ElementTree.ElementTree() :\n",
    "    \"\"\"\n",
    "        Creates an XML tree from a dictionary wrapped around root tag.\n",
    "\n",
    "        Args:\n",
    "        parameter_dictionary : Dictionary which will be converted to xml tree.\n",
    "        root_tag : Tag around which elements of dictionary will be wrapped.\n",
    "                    Defaults to \"layer\".\n",
    "    \n",
    "        Returns : ElementTree.ElementTree() object.\n",
    "    \"\"\"\n",
    "    layer = ElementTree.Element(root_tag)\n",
    "    for parameter_desc in parameter_dictionary :\n",
    "        parameter_description = ElementTree.Element(parameter_desc)\n",
    "        parameter_description.text = str(parameter_dictionary[parameter_desc])\n",
    "        layer.append(parameter_description)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xml_file(parameter_dictionary : dict,\n",
    "                    xml_path : str,\n",
    "                    root_tag : str,\n",
    "                    element_tag : str) -> int :\n",
    "    \"\"\"\n",
    "        Appends layer description to xml file and if xml doesn't exist or is empty, \n",
    "        creates an xml file with required headers.\n",
    "\n",
    "        Args:\n",
    "        parameter_dictionary : Dictionary containing layer description.\n",
    "        xml_path : Path where xml file will be stored / created.\n",
    "        root_tag : Tag around which xml file will be wrapped.\n",
    "        element_tag : Tag around which each element in dictionary will be wrapped.\n",
    "    \"\"\"\n",
    "   \n",
    "    if not os.path.exists(xml_path) :\n",
    "        # Create base xml file.\n",
    "        f = open(xml_path, \"w\")\n",
    "        data = \"<\" + root_tag + \">\" + \"</\" + root_tag + \">\"\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "    layer_description = create_xml_tree(parameter_dictionary, element_tag)\n",
    "    xml_file = ElementTree.parse(xml_path)\n",
    "    root = xml_file.getroot()\n",
    "    layer = root.makeelement(element_tag, parameter_dictionary)\n",
    "    root.append(layer_description)\n",
    "    xml_file.write(xml_path, encoding = \"unicode\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_layers(modules, xml_path, base_path, layer_index, debug : bool) -> int :\n",
    "    \"\"\"\n",
    "        Parses model and generates csv and xml file which will be iterated by C++ translator.\n",
    "    \n",
    "        Args:\n",
    "        modules : PyTorch model for which parameter csv and xml will be created.\n",
    "        xml_path : Directory where xml with model config will be saved.\n",
    "        base_path : Directory where csv will be stored.\n",
    "\n",
    "        Returns 0 if weights are created else return 1.\n",
    "    \"\"\"\n",
    "    for block in modules :\n",
    "        for layer in block :\n",
    "            layer_index += 1\n",
    "            parameter_dict = extract_weights(layer, str(layer_index), base_path)\n",
    "            create_xml_file(parameter_dict, xml_path, \"model\", \"layer\")\n",
    "            if not os.path.exists(parameter_dict[\"weight_csv\"]) and parameter_dict[\"has_weights\"] == 1:\n",
    "                print(\"Creating weights failed!\")\n",
    "                return 1, layer_index\n",
    "            if debug :\n",
    "                print(\"Weights created succesfully for \", parameter_dict[\"name\"], \" layer index :\", layer_index)\n",
    "    return 0, layer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_model(model, xml_path, base_path, debug : bool) -> int :\n",
    "    \"\"\"\n",
    "        Parses model and generates csv and xml file which will be iterated by C++ translator.\n",
    "    \n",
    "        Args:\n",
    "        model : PyTorch model for which parameter csv and xml will be created.\n",
    "        xml_path : Directory where xml with model config will be saved.\n",
    "        base_path : Directory where csv will be stored.\n",
    "\n",
    "        Returns 0 if weights are created else return 1.\n",
    "    \"\"\"\n",
    "    layer_index = 0\n",
    "    error, layer_index = iterate_over_layers(model.features, xml_path, base_path, layer_index, debug)\n",
    "    if error :\n",
    "        print(\"An error occured!\")\n",
    "        return 1\n",
    "    print(layer_index)\n",
    "    error, layer_index = iterate_over_layers(model.classifier, xml_path, base_path, layer_index, debug)\n",
    "    if error :\n",
    "        print(\"An error occured!\")\n",
    "        return 1\n",
    "    print(layer_index)\n",
    "    if debug :\n",
    "        print(\"Model weights saved! Happy mlpack-translation.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Weights created succesfully for  Convolution2D  layer index : 1\nWeights created succesfully for  BatchNorm2D  layer index : 2\nWeights created succesfully for  unknown_layer  layer index : 3\nWeights created succesfully for  unknown_layer  layer index : 4\nWeights created succesfully for  Convolution2D  layer index : 5\nWeights created succesfully for  BatchNorm2D  layer index : 6\nWeights created succesfully for  unknown_layer  layer index : 7\nWeights created succesfully for  unknown_layer  layer index : 8\nWeights created succesfully for  Convolution2D  layer index : 9\nWeights created succesfully for  BatchNorm2D  layer index : 10\nWeights created succesfully for  unknown_layer  layer index : 11\nWeights created succesfully for  Convolution2D  layer index : 12\nWeights created succesfully for  BatchNorm2D  layer index : 13\nWeights created succesfully for  unknown_layer  layer index : 14\nWeights created succesfully for  Convolution2D  layer index : 15\nWeights created succesfully for  BatchNorm2D  layer index : 16\nWeights created succesfully for  unknown_layer  layer index : 17\nWeights created succesfully for  unknown_layer  layer index : 18\nWeights created succesfully for  Convolution2D  layer index : 19\nWeights created succesfully for  BatchNorm2D  layer index : 20\nWeights created succesfully for  unknown_layer  layer index : 21\nWeights created succesfully for  Convolution2D  layer index : 22\nWeights created succesfully for  BatchNorm2D  layer index : 23\nWeights created succesfully for  unknown_layer  layer index : 24\nWeights created succesfully for  Convolution2D  layer index : 25\nWeights created succesfully for  BatchNorm2D  layer index : 26\nWeights created succesfully for  unknown_layer  layer index : 27\nWeights created succesfully for  unknown_layer  layer index : 28\nWeights created succesfully for  Convolution2D  layer index : 29\nWeights created succesfully for  BatchNorm2D  layer index : 30\nWeights created succesfully for  unknown_layer  layer index : 31\nWeights created succesfully for  Convolution2D  layer index : 32\nWeights created succesfully for  BatchNorm2D  layer index : 33\nWeights created succesfully for  unknown_layer  layer index : 34\nWeights created succesfully for  Convolution2D  layer index : 35\nWeights created succesfully for  BatchNorm2D  layer index : 36\nWeights created succesfully for  unknown_layer  layer index : 37\nWeights created succesfully for  Convolution2D  layer index : 38\nWeights created succesfully for  BatchNorm2D  layer index : 39\nWeights created succesfully for  unknown_layer  layer index : 40\nWeights created succesfully for  Convolution2D  layer index : 41\nWeights created succesfully for  BatchNorm2D  layer index : 42\nWeights created succesfully for  unknown_layer  layer index : 43\nWeights created succesfully for  unknown_layer  layer index : 44\nWeights created succesfully for  Convolution2D  layer index : 45\nWeights created succesfully for  BatchNorm2D  layer index : 46\nWeights created succesfully for  unknown_layer  layer index : 47\nWeights created succesfully for  Convolution2D  layer index : 48\nWeights created succesfully for  BatchNorm2D  layer index : 49\nWeights created succesfully for  unknown_layer  layer index : 50\nWeights created succesfully for  Convolution2D  layer index : 51\nWeights created succesfully for  BatchNorm2D  layer index : 52\nWeights created succesfully for  unknown_layer  layer index : 53\nWeights created succesfully for  Convolution2D  layer index : 54\nWeights created succesfully for  BatchNorm2D  layer index : 55\nWeights created succesfully for  unknown_layer  layer index : 56\nWeights created succesfully for  Convolution2D  layer index : 57\nWeights created succesfully for  BatchNorm2D  layer index : 58\nWeights created succesfully for  unknown_layer  layer index : 59\n59\nWeights created succesfully for  Convolution2D  layer index : 60\nWeights created succesfully for  unknown_layer  layer index : 61\nWeights created succesfully for  unknown_layer  layer index : 62\n62\nModel weights saved! Happy mlpack-translation.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "parse_model(model, \"model_config.xml\", \"./weights/darknet19/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}